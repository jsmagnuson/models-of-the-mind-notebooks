{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cc802c-41bc-4405-90e3-6fbecaa4ed0f",
   "metadata": {},
   "source": [
    "## Preliminaries: Understanding Logical Symbols in Probability\n",
    "\n",
    "Understanding the symbols used in probability is crucial for reading and interpreting mathematical statements. Below is a brief guide on how to read some of the most common logical symbols in the context of probability.\n",
    "\n",
    "### Identity or truth \n",
    "\n",
    "If event A has occurred, we simply state 'A'. This can be a little confusing. Let's compare it to *not*.\n",
    "\n",
    "### Logical NOT: $\\neg$ or $\\text{not}$\n",
    "\n",
    "The symbol $\\neg$ or the word \"not\" is used to denote the complement of an event A. Think of many possible states of the world, where in some of them, A is true (A has happened), while in others, A has not happened. These two sets are complements of each other, because we are saying for all states we care about, A will either happen or not. For example, we could talk about A being \"my bus arrived on time\" but not A ($\\neg A$) as \"my bus did not arrive on time\". For all times you took the bus, one of those is true, but not both. We can then talk about the probabilty of $A$ and $\\neg A$. \n",
    "\n",
    "Let's suppose that over the last year, I rode the bus 100 times, and it was on time 61 times. So the probability of A is 0.61, while the probability of $\\neg A$ would be 1 - 0.61 = 0.39. Note that $A + \\neg A = 1.0$, which is why they are complements. \n",
    "\n",
    "So $ P(\\neg A) $ should be read as \"the probability of not A\" or \"the probability of the complement of A\"; conceptually, again, this denotes the probabilty of A not happening. \n",
    "\n",
    "\n",
    "### Conditional Probability: $|$\n",
    "\n",
    "The vertical bar $|$ is used to denote conditional probability. It reads as \"given.\" For example, $ P(A | B)$ should be read as \"the probability of A given B.\" When we say B is \"given\" we mean it is true. So \"the probability of A given B\" means the probability of A when we assume B has happened. For example, we could talk about the probability of experiencing rain when the morning weather report tells us to expect rain. That is, we can talk about the probability of it actually raining *given* that the weather report said it would rain.\n",
    "\n",
    "### Logical AND: $\\land$ or $\\text{and}$\n",
    "\n",
    "The symbol $\\land$ or the word \"and\" is used to denote the intersection of two events A and B. For example, $ P(A \\land B) $ should be read as \"the probability of A **and** B\" (that is, the probability that both are true). \n",
    "\n",
    "We could also make a rule using this form. For example, *only proceed if* $ A \\land B $. We could see this in real life if we had a rule that a faculty meeting could only proceed if *both* the department head **and** the associate department head are present (A = department head is present, B = associate head is present; we can only proceed when A and B are both true, that is, both the head and associate head are present).\n",
    "\n",
    "### Logical OR: $\\lor$ or $\\text{or}$\n",
    "\n",
    "The symbol $\\lor$ or the word \"or\" is used to denote the *union* of two events A and B -- that is, all cases where either one of them is true. As sets, we would describe the cases where $A \\lor B$ is true to be: $\\{A, B\\}, \\{A, \\neg B\\}, \\{\\neg A, B\\}$. $A \\lor B$ is not true for $\\{\\neg A, \\neg B\\}$. \n",
    "\n",
    "So $ P(A \\lor B) $ should be read as \"the probability of A **or** B.\"\n",
    "\n",
    "We could see this in real life if we our faculty meeting rule was that the meeting could only proceed if *either* the department head **or** the associate department head is present, or if both are present (A = department head is present, B = associate head is present; we can only proceed when at least A or B [or both] is   true).\n",
    "\n",
    "\n",
    "### Exclusive OR (XOR): $\\veebar$\n",
    "\n",
    "The symbol $\\veebar$ denotes the exclusive OR, which means either A or B but not both.\n",
    "\n",
    "As sets, we would describe the cases where $A \\veebar B$ is true to be: $\\{A, \\neg B\\}, \\{\\neg A, B\\}$. $A \\veebar B$ is not true for $\\{\\neg A, \\neg B\\}$ or for $\\{A, B\\}$ (that is, $ A \\land B $). \n",
    "\n",
    "So $ P(A \\veebar B) $ should be read as \"the probability of A XOR B\" (you really say \"ex or\"). \n",
    "\n",
    "---\n",
    "### Logic in programming\n",
    "\n",
    "These notions are also important in programming. Let's look at them in Python, which has very transparent logical operators. \n",
    "\n",
    "#### Understanding Logic Operators in Python\n",
    "\n",
    "In Python, logical operators are used to perform logical operations on variables and values. These operators evaluate expressions to `True` or `False`. These are known as *Boolean* logical comparisons, named for George Boole (1815-64), 'a founder of the algebraic tradition in logic.'\n",
    "\n",
    "## `and` Operator\n",
    "\n",
    "The `and` operator returns `True` if both expressions are true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbd30e7-f7a7-48e0-b9a9-15fee184cd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = True\n",
    "y = False\n",
    "result = x and y  # This will return False\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b40b72-e383-4af3-926e-8750a7f7e192",
   "metadata": {},
   "source": [
    "## `or` Operator\n",
    "\n",
    "The `or` operator returns `True` if at least one of the expressions is true (or if both are true).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b4ac69f-fafd-4636-8065-e8875848eaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = True\n",
    "y = False\n",
    "result = x or y  # This will return True\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7acef5e1-b5f9-49a8-bd76-d8e30cf9269a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = True\n",
    "y = True\n",
    "result = x or y  # This will return True\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff7fb22d-2d4b-4cbd-8816-29dfad626598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = False\n",
    "y = False\n",
    "result = x or y  # This will return False\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89cbfa3-510f-430e-95bc-62a053a90170",
   "metadata": {},
   "source": [
    "## `not` Operator\n",
    "\n",
    "The `not` operator inverts the value of the expression following it. If the expression is `True`, `not` makes it `False` and vice versa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff95cb9-d112-41a6-87e0-d3ec71994d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False, True\n"
     ]
    }
   ],
   "source": [
    "x = True\n",
    "result = not x  # This will return False\n",
    "y = False\n",
    "result2 = not y  # This will return True\n",
    "print(f'{result}, {result2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da8bb0-b27b-4d14-844b-839f2fdd72ce",
   "metadata": {},
   "source": [
    "## Comparison Operators\n",
    "\n",
    "Python also includes comparison operators (`==`, `!=`, `<`, `<=`, `>`, `>=`) that return Boolean values (`True` or `False`). '==' is \"is equal to\", \"!=\" is \"not equal to\", '<' is less than, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1cdafda-0756-4d76-9dbf-0eb0deef5d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 5\n",
    "y = 10\n",
    "result = x == y  # This will return False\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43083f6a-6c53-4476-8da5-662b1d8a0fcd",
   "metadata": {},
   "source": [
    "## Chaining Logical Operators\n",
    "\n",
    "You can chain multiple logical operators together to form more complex conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23865faf-f171-45d1-8369-cf8ed2f32905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 5\n",
    "y = 10\n",
    "result = (x == y)  # This will return False\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8346c2-f46c-46f3-96e8-249fcd4c3e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 5\n",
    "y = 10\n",
    "z = 20\n",
    "result = x < y and y < z  # This will return True\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d8745-f04c-4daf-8f1e-107181bfe691",
   "metadata": {},
   "source": [
    "Though it's a good practice to use parentheses to keep things clear (some programmers will scoff, but I think it's helpful). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "258e1851-fd7f-41ca-a076-3133b5cbe168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First is False, second is True\n"
     ]
    }
   ],
   "source": [
    "x = 5\n",
    "y = 10\n",
    "result1 = (x == y)  # This will return False\n",
    "\n",
    "x = 5\n",
    "y = 10\n",
    "z = 20\n",
    "result2 = ((x < y) and (y < z))  # This will return True\n",
    "\n",
    "print(f' First is {result1}, second is {result2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7732f41-ee0e-41ed-a8e6-05c9027fdf34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Understanding these logical operators is essential for control flow in Python programming. \n",
    "They allow us to create complex conditions and decision-making structures. They can also \n",
    "really help you understand probability as you get used to using them. \n",
    "\n",
    "**Okay, let's move on to more advanced topics.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd53e2-17cd-4a8f-b008-40555372a628",
   "metadata": {},
   "source": [
    "## Key Concepts in Probability\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Probability is the branch of mathematics that deals with quantifying uncertainty. It's the toolset we use to describe phenomena where the outcome is not deterministic but can be any of a range of possibilities. Understanding probability is essential for fields like statistics, psychology, neuroscience, finance, and machine learning. This overview will equip you with the foundational knowledge required for a deeper understanding of Bayes' Theorem, which we will discuss in the next notebook.\n",
    "\n",
    "#### Example: Weather Forecasting\n",
    "\n",
    "In weather forecasting, we often hear terms like \"30% chance of rain,\" which is an application of probability. The forecast doesn't tell us definitively whether it will rain; instead, it quantifies the likelihood. It may help to think about probabilities as *relative frequencies*. A 30% chance or rain means something like \"out of 100 days with conditions like today's conditions, it will rain on 30 of them\" or \"out of 10,000 days with conditions like today's conditions, it will rain on 3,000 of them\".\n",
    "\n",
    "---\n",
    "\n",
    "### Sample Space $(S)$\n",
    "\n",
    "The sample space is the set of all possible outcomes (usually of an *experiment*, which can be as simple as tossing a coin).\n",
    "\n",
    "#### Example: Flipping a coin\n",
    "\n",
    "If you toss a coin, there are two possible outcomes, heads or tails. We describe the sample space as $S= \\{0, 1\\}$, where we assign one outcome the value of 0 and the other the value of 1.\n",
    "\n",
    "#### Example: Rolling a Die\n",
    "\n",
    "If you roll a standard six-sided die, the sample space $S = \\{1, 2, 3, 4, 5, 6\\}$ (or more consistent with the previous example, but very confusing for human readability, $S = \\{0, 1, 2, 3, 4, 5\\}$).\n",
    "\n",
    "---\n",
    "\n",
    "### Event\n",
    "\n",
    "An event is a subset of the sample space. An event $A$ occurs if the outcome of the experiment is an element of $S$.\n",
    "\n",
    "#### Example: Scoring High on a Die Roll\n",
    "\n",
    "Events can be simple outcomes, like rolling a 6. But they can also be sets themselves. For example, we could define an event $A$ as \"rolling a number greater than 4\" on a standard six-sided die. Then $A = \\{5, 6\\}$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7a2c1-3b6e-4262-9d47-d40f921f45c8",
   "metadata": {},
   "source": [
    "\n",
    "### Probability Measure $(P)$\n",
    "\n",
    "The Probability Measure assigns a number between 0 and 1 to each event. $A$ represents an event and $S$ represents the sample space. The probability of the entire sample space $S$ is 1, and the probability of impossible events is 0.\n",
    "\n",
    "$$\n",
    "P(A) \\geq 0 \\quad \\text{and} \\quad P(S) = 1\n",
    "$$\n",
    "\n",
    "#### Example: Lottery Ticket\n",
    "\n",
    "If you buy a lottery ticket, the probability of winning the jackpot might be $1$ in 10,000,000 or $P(A) = \\frac{1}{10,000,000}$. The probability of ***not*** winning is very close to 1.\n",
    "\n",
    "#### Example: Raffle\n",
    "\n",
    "At a town celebration, there is a raffle. There are 100 tickets. Only 4 people buy tickets. Bob buys 50, Sally buys 25, Renatta buys 15, and Jose buys 10. They write their names on each ticket, and then the tickets are put in a hat and shaken up. The mayor draws a ticket. What are the possible outcomes? The winner will be Bob, Sally, Renatta, or Jose. So there are 4 possible outcomes. Their probabilties (since we have this convenient total number of 100 tickets) are 0.5, 0.25, 0.15, and 0.1. These have to sum to 1.0, just like the number of tickets in the hat has to be the same as the number of tickets sold.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Priors\n",
    "\n",
    "In probability and statistics, a \"prior\" is a pre-existing belief about an uncertain situation. Essentially, it's your best estimate (or guess) before considering new evidence. Priors can be based on historical data, expert opinion, or some other form of background knowledge.\n",
    "\n",
    "#### Example: Fair Coin\n",
    "\n",
    "If you assume that a coin is \"fair,\" your prior for the probability of getting heads is $0.5$, and likewise for tails. This is based on the belief that a fair coin has equal chances for heads and tails. \n",
    "\n",
    "#### Informal Testing\n",
    "\n",
    "To informally test whether a coin is fair, you might simply flip it several times and observe the outcomes. If the coin is indeed fair, you would expect to see approximately as many heads as tails over a large number of flips. However, this method is not precise unless you flip the coin a large or even an enormous number of times...\n",
    "\n",
    "#### Statistical Testing with Binomial Distribution\n",
    "\n",
    "For a more rigorous assessment, you can use the statistics of *binomial* distributions (cases where there are only 2 possible events or outcomes). If you flip the coin $n$ times and observe $k$ heads, the probability of this happening with a fair coin can be calculated using the binomial probability formula:\n",
    "\n",
    "$$\n",
    "P(k \\text{ heads in } n \\text{ flips}) = \\binom{n}{k} \\times 0.5^k \\times 0.5^{(n-k)}\n",
    "$$\n",
    "\n",
    "Here, $\\binom{n}{k}$ is the binomial coefficient, representing the number of ways to get $k$ successes in $n$ trials. See more on this below.\n",
    "\n",
    "After calculating this probability, if the result is extremely low, you may have grounds to reject the hypothesis that the coin is fair. Typically, a threshold (often 5% or 1%) is set for this purpose. Note that our estimate will become more precise as we increase the number of flips. (Also, the threshold depends on context. For example, if we are minting a new coin for public use as currency, we might be satisfied with a coin that is close to but not perfectly weighted. However, if we are in charge of selecting coins to use for a professional football coin toss (the winner gets to choose to kick off or receive), we would want it to be perfectly weighted. Think about real-life examples where the *stakes* are different.)\n",
    "\n",
    "**Understanding the Binomial Coefficient in the Binomial Probability Formula**\n",
    "\n",
    "In the formula for calculating the probability of observing $k$ heads in $n$ flips of a fair coin,\n",
    "\n",
    "$$\n",
    "P(k \\text{ heads in } n \\text{ flips}) = \\binom{n}{k} \\times 0.5^k \\times 0.5^{(n-k)}\n",
    "$$\n",
    "\n",
    "The term $\\binom{n}{k}$ is, again, the \"binomial coefficient.\" This term represents the number of different ways you can\n",
    "choose $k$ successes (in this case, heads) out of $n$ trials (coin flips).\n",
    "\n",
    "$\\binom{n}{k}$ tells you how many different sequences of heads and tails will result in exactly $k$ heads when you flip a \n",
    "coin $n$  times. For example, if $n = 3$ and $k= 2$, then $\\binom{3}{2} = 3$ because there are three different ways to get exactly 2 heads in 3 coin flips: HHT, HTH, or THH.\n",
    "\n",
    "The binomial coefficient is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "\\binom{n}{k} = \\frac{n!}{k! \\times (n-k)!}\n",
    "$$\n",
    "\n",
    "Here, $n!$ denotes the factorial of $n$, which is the product of all positive integers up to $n$ (e.g., if $n=3$, $n! = 1 \\times 2 \\times 3 = 6$).\n",
    "\n",
    "**Role in the Binomial Probability Formula**\n",
    "\n",
    "The binomial coefficient serves as a \"weight\" that accounts for the different ways $k$ successes can happen in $n$ trials, making it a crucial part of the binomial probability formula.\n",
    "\n",
    "\n",
    "***Probability of 4 Heads in 4 Flips with a Fair Coin***\n",
    "\n",
    "Let's try calculating the probability of getting 4 heads in a row with a fair coin. Again, we use the binomial probability formula:\n",
    "\n",
    "$$\n",
    "P(4 \\text{ heads in 4 flips}) = \\binom{4}{4} \\times 0.5^4 \\times 0.5^{(4-4)}\n",
    "$$\n",
    "\n",
    "Here, $\\binom{4}{4}$ is the binomial coefficient. Let's first calculate what this resolves to, plugging it into our formula. \n",
    "\n",
    "$$\n",
    "\\binom{4}{4} = \\frac{4!}{4!(4-4)!}\n",
    "$$\n",
    "\n",
    "The result of this calculation is $1$. This means there is exactly one way to choose 4 items out of 4 (this will be true for any case where $n = k$).\n",
    "\n",
    "\n",
    "\n",
    "The probability of getting a head in each individual flip is 0.5, and since we are considering 4 flips, this becomes $0.5^4$. So now what happens if we just calculate $1 \\times 0.5^4 \\times 0.5^{(4-4)}$? We will get 0.0625. So now let's wrap this up: \n",
    "\n",
    "$$\n",
    "P(4 \\text{ heads in 4 flips}) = 1 \\times 0.0625 \\times 1 = 0.0625\n",
    "$$\n",
    "\n",
    "So, there is a 6.25% chance of getting 4 heads in a row with a fair coin. While 6.25% is small, it's maybe not as small as we might have expected. We should discuss low-probability events in class. \n",
    "\n",
    "Let's do it in python in the code block below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3b8b29c-3476-49b2-b1d1-ebc55e4d8373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import comb\n",
    "\n",
    "# Number of trials (n) and number of successes (k)\n",
    "n = 4  # Number of coin flips\n",
    "k = 4  # Number of heads\n",
    "\n",
    "# Probability of getting heads or tails with a fair coin\n",
    "p_head = 0.5\n",
    "\n",
    "# Calculate the probability using the binomial distribution formula\n",
    "probability = comb(n, k) * (p_head ** k) * ((1 - p_head) ** (n - k))\n",
    "probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306a86f-3793-412e-8da2-76bb5f1e2547",
   "metadata": {},
   "source": [
    "---\n",
    "#### Example: Fair Six-Sided Die\n",
    "\n",
    "Similarly, if you have a standard six-sided die and assume that it's \"fair,\" your prior for each outcome from 1 to 6 would be $\\frac{1}{6}$. This is based on the symmetry and equal weight distribution of a standard six-sided die.\n",
    "\n",
    "---\n",
    "\n",
    "### Updating Priors\n",
    "\n",
    "Priors are updated with new data through the use of conditional probabilities and Bayes' Theorem. After the update, the prior becomes a \"*posterior* probability,\" which is a refined probability estimate that incorporates the new evidence (so we have prior and posterior, or probabities estimated before and after new evidence is available). This process of updating is the cornerstone of Bayesian inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Conditional Probability: $P(A|B)$\n",
    "\n",
    "Conditional probability is the probability of $A$ occurring given that $B$ has already occurred.\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\land B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Again, we read the part of this on the left side of the = sign as \"the probability of A given B\", which means we are considering the probability of A being true if we assume B is true. We'd read the whole thing as \"the probability of A given B equals the probability of A and B occurring at the same time divided by the overall probability of B.\"\n",
    "\n",
    "#### Example: Medical Diagnosis\n",
    "\n",
    "If a person has certain symptoms (event $B$), the probability they have a particular disease (event $A$) might be higher, given those symptoms. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Independence\n",
    "\n",
    "### Example: Coin Tosses\n",
    "\n",
    "If you toss a coin twice, the outcome of the first toss is obviously independent of the outcome of the second toss -- but the same is true of the second toss (it is unaffected by the outcome of the first toss). The coin has no memory, of course!\n",
    "\n",
    "### A more complex example\n",
    "\n",
    "Events $A$ and $B$ are independent if the occurrence of one does not affect the occurrence of the other.\n",
    "\n",
    "$$\n",
    "P(A \\land B) = P(A) \\times P(B)\n",
    "$$\n",
    "\n",
    "How do we read this? The probability of A **and** B is the probability of A times the probability of B. For example, if I decide I will walk to work every other Monday during the fall semsester, the probability of me walking to work on a Monday is 0.5. However, this has no impact on the probability that it will rain on Mondays (or any other day!). So if the probability of daytime rain in the fall semester is 0.1 (1 day out of 10), the probability that on a given Monday, I will walk to work **and** it will rain is 0.5 * 0.1 = 0.05 (5%, or, on average, 5 out of 100 or 1 out of 20 Mondays).\n",
    "\n",
    "In contrast, if I'm only 20% likely to walk to work if it's raining, these are not independent anymore. Now whether I walk to work is affected by whether it is raining. So now we have to treat rainy and non-rainy Mondays separately. If 10% of Mondays are rainy, 90% are not. So if we were considering 100 Mondays, on average 90 would not be rainy, and on those days there would be a 50% chance I would walk -- that's 45 out of 90 non-rainy Mondays. For the other 10 Mondays (the rainy ones), I would only be 20% likely to walk on those days, so that would be 2 out of 10 (on average). \n",
    "\n",
    "So now we have (0.9 * 0.5) + (0.1 * 0.2) = 0.45 + 0.02 = 0.47 chance of me walking on Mondays when we take the causal effect of rain on my behavior into account. Note that when we calculate this, which factor is causal is not indicated. For an interesting approach to taking causality into account, see Judea Pearl's work on \"do calculus\".  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51423ca9-141c-451a-b1dc-999efc22c9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65e68d-3a20-4231-b491-34086f0ad6f7",
   "metadata": {},
   "source": [
    "## When to Multiply and When to Add Probabilities\n",
    "\n",
    "### Multiplication Rule\n",
    "\n",
    "You multiply probabilities when events are independent.\n",
    "\n",
    "$$\n",
    "P(A  \\land B) = P(A) \\times P(B)\n",
    "$$\n",
    "\n",
    "#### Example: Cards and/or coins\n",
    "\n",
    "Getting 2 heads in a row flipping a fair coin = $0.5 \\times 05$. \n",
    "\n",
    "---\n",
    "\n",
    "What about drawing cards from a shuffled deck? What are the chances of getting an Ace and *then* a King? Intuitively, it might seem like it should (4/52) x (4/52) since there are 4 Aces and 4 Kings. However, for this to be corrrect, we would have to draw the Ace and then replace it in the deck and then reshuffle before drawing the second card. In that case, yes, the probabilities are independent, just like coin tosses, so we would get (4/52) x (4/52) = 0.0059 (0.59%). \n",
    "\n",
    "However, typically, we would just draw the first two cards. So now if we want to ask what is the probability of an Ace and *then* a King *without replacing the Ace*, it seems like we should adjust slightly: (4/52) x (4/51) = 0.006 (0.6%, or 6 out of 1000 -- very slightly more probable, with the second denominator changed to 51 because that's how many cards are left in the deck)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ed815-0571-4200-b0cc-b8b51015017b",
   "metadata": {},
   "source": [
    "Let's try to work this out another way. First, let's just consider the *unordered* probability of getting a King and an Ace (either Ace-King or King-Ace). To calculate the probability of an Ace and a King being the first 2 cards in either order, we have to think about how many 2-card combinations there are. There are (52 x 51)/2 = 1326 combinations. Of those, 16 would be Ace-King combinations (since there are 4 Aces and 4 Kings). So if you draw two cards from a deck, the probability that 1 is an Ace and 1 is a king is 16/1326 = 0.012 (1.2%). Now what about the ordered probability? Of the 16 combinations, 8 will have Ace first, so the probability of getting an Ace and then a King is 8/1326 = 0.006 (0.6% or 6 out of 1000).\n",
    "\n",
    "---\n",
    "\n",
    "### Addition Rule\n",
    "\n",
    "You add probabilities when considering mutually exclusive events.\n",
    "\n",
    "$$\n",
    "P(A \\lor B) = P(A) + P(B)\n",
    "$$\n",
    "\n",
    "#### Example: Multiple Choice Quiz\n",
    "\n",
    "In a multiple-choice question with 4 choices, where only one answer is correct, the probability of guessing correctly is $P(A) = \\frac{1}{4}$. The probability of guessing incorrectly is $P(B) = \\frac{3}{4}$. The sum $P(A) + P(B) = 1$ covers all possible outcomes for one question.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dee7e2-9651-44b3-978f-0da9da9952df",
   "metadata": {},
   "source": [
    "### Optional advanced topic: \n",
    "#### Using Logarithms to Simplify Probability Computations\n",
    "\n",
    "In probability and statistics, you often encounter products of probabilities, especially in scenarios involving independent events or likelihood functions. These products can quickly become too small or too large to handle, causing numerical instability in calculations. Logarithms provide a way to simplify these computations.\n",
    "\n",
    "#### Why Use Logarithms?\n",
    "\n",
    "Logarithms convert multiplications into additions and divisions into subtractions. This makes the arithmetic more manageable and helps to avoid numerical issues like underflow (extremely small values) and overflow (extremely large values).\n",
    "\n",
    "$$\n",
    "\\log(a \\times b) = \\log(a) + \\log(b)\n",
    "$$\n",
    "$$\n",
    "\\log\\left(\\frac{a}{b}\\right) = \\log(a) - \\log(b)\n",
    "$$\n",
    "\n",
    "##### Example: Independent Events\n",
    "\n",
    "Suppose we want to calculate the probability of observing a specific sequence in a Bernoulli (binomial) trial with success probability $ p $ and failure probability $ 1 - p $.\n",
    "\n",
    "The probability of observing, say, \"success, failure, success\" (or heads, tails, heads) would be $ p \\times (1-p) \\times p $.\n",
    "\n",
    "Using logarithms, we can simplify this to:\n",
    "\n",
    "$$\n",
    "\\log(p \\times (1-p) \\times p) = \\log(p) + \\log(1-p) + \\log(p)\n",
    "$$\n",
    "\n",
    "To get it back to a probabilty, we would exponate the result, e.g.: \n",
    "\n",
    "$$\n",
    "z' = \\log(p) + \\log(1-p) + \\log(p)\n",
    "$$\n",
    "\n",
    "Now $d$ is the log probability of observing the sequence. We get the probability by converting it back:\n",
    "\n",
    "$$\n",
    "z = exp(z')\n",
    "$$\n",
    "\n",
    "Now $z$ is the sequence probability. \n",
    "\n",
    "### Numerical Example: Using Probabilities vs Logarithms\n",
    "\n",
    "Let's consider a numerical example where the probability of success is 0.7 and the probability of failure is 0.3. This could mean a weighted coin where the probability of heads is 0.7, a quality control scenario (where a worker detecting a defective product could be 'success'), or a medical testing scenario (where 0.7 is the probability of detecting disease if the patient has it).\n",
    "\n",
    "#### Using Probabilities\n",
    "\n",
    "The probability of the sequence \"success, failure, success\" can be calculated as follows:\n",
    "\n",
    "$$\n",
    "0.7 \\times 0.3 \\times 0.7 = 0.147\n",
    "$$\n",
    "\n",
    "#### Using Logarithms\n",
    "\n",
    "We can also calculate the log-probability of the same sequence:\n",
    "\n",
    "$$\n",
    "\\log(0.7) + \\log(0.3) + \\log(0.7) \\approx -1.917\n",
    "$$\n",
    "\n",
    "#### Converting Back to Original Probability Scale\n",
    "\n",
    "To convert the log-probability back to the original probability scale, we take the exponent:\n",
    "\n",
    "$$\n",
    "\\exp(-1.917) \\approx 0.147\n",
    "$$\n",
    "\n",
    "Both methods yield the same result, confirming the equivalence of the two approaches. Using logarithms can be particularly useful in practice for avoiding numerical underflow or overflow and for computational efficiency.\n",
    "\n",
    "#### Now let's do this in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7ca103b-4b90-49b9-a459-fceb1c9921b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 0.147000, Log prob: -1.917323, Exp log prob: 0.147000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Given probabilities\n",
    "p_success = 0.7  # Probability of success\n",
    "p_failure = 0.3  # Probability of failure (1 - p_success)\n",
    "\n",
    "# Calculate the probability of the sequence \"success, failure, success\"\n",
    "probability = p_success * p_failure * p_success\n",
    "\n",
    "# Calculate the log-probability of the sequence \"success, failure, success\"\n",
    "log_probability = math.log(p_success) + math.log(p_failure) + math.log(p_success)\n",
    "\n",
    "# Convert the log-probability back to the original probability scale\n",
    "exp_log_probability = math.exp(log_probability)\n",
    "\n",
    "print(f'Probability: {probability:.6f}, Log prob: {log_probability:.6f}, \\\n",
    "Exp log prob: {exp_log_probability:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773819e1-83d6-4b26-bc33-9b1b87282494",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Advantages of working with logarithms\n",
    "\n",
    "These advantages accrue when you are working with many computations; this may be relevant if you work with complex Bayesian models or neural network analyses.\n",
    "\n",
    "1. **Computational Stability**: Log-probabilities mitigate the risk of numerical underflow (calculating values too small to represent on your computer), which can occur when multiplying many small probabilities together.\n",
    "2. **Speed**: Addition is generally faster than multiplication in terms of computational time.\n",
    "3. **Summation**: When dealing with products of probabilities, converting to log-probabilities allows you to use summation techniques, which can be more efficient.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Logarithms are a powerful tool for simplifying complex probability calculations. They are especially useful in scenarios involving the multiplication of many probabilities, where numerical issues are likely to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984ab6fb-a830-41b8-98ed-70707f437565",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Understanding these foundational concepts is crucial for diving deeper into the world of probability and statistics, especially as they lay the groundwork for advanced topics like Bayes' Theorem. These principles are not just theoretical; they are used in everyday decision-making (though maybe not as often as they should be...) and in various scientific disciplines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
