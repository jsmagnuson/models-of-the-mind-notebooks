




















import numpy as np
import random
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import time 
from IPython.display import clear_output





def plot_training_progress(loss_history, word_choices_history, prop_better_history, \
                           combined_input_target_output_matrix, combined_hidden_context_matrix, \
                           W_input_hidden, W_context_hidden, W_hidden_output, epoch, loss_lexicon_history=None):

    clear_output(wait=True)
    fig = plt.figure(figsize=(15, 6))

    # Create a GridSpec for layout
    # gs = fig.add_gridspec(5, 3, width_ratios=[2, 1, 0.1], height_ratios=[3, 2, 5, 5, 5])
    gs = fig.add_gridspec(5, 4, width_ratios=[4, 4, 2, 0.1], height_ratios=[3, 2, 5, 5, 5])

    # Plot loss
    ax_loss = fig.add_subplot(gs[:, 0])
    ax_loss.plot(range(1, len(loss_history) + 1), loss_history, label='Training')
    if loss_lexicon_history is not None:
        # Plot red points for loss_lexicon (test loss)
        epochs_with_test = [i for i, loss in enumerate(loss_lexicon_history) if not np.isnan(loss)]
        test_losses = [loss for loss in loss_lexicon_history if not np.isnan(loss)]
        ax_loss.scatter(epochs_with_test, test_losses, color='violet', s=25, label='Testing', zorder=5)
    # Adding grey dashed lines at y=0
    ax_loss.axhline(y=0, color='lightgrey', linestyle='--')
    ax_loss.set_xlabel('Epoch')
    ax_loss.set_ylabel('Cross-Entropy Loss')
    ax_loss.set_title(f'Loss vs Epochs, EPOCH {epoch}')
    ax_loss.legend()
    
    # New subplot for proportion_better and word_choices
    ax_tests = fig.add_subplot(gs[:, 1])
    if prop_better_history is not None:
        #ax_tests.plot(range(1, len(prop_better_history) + 1), prop_better_history, 'r.', markersize=10, label='Proportion Better')
        # Plotting both points and a connecting line
        # ax_tests.plot(range(1, len(prop_better_history) + 1), prop_better_history, 'r.-', label='Proportion Better', markersize=5)  # Connecting line with points
        epochs = range(1, len(prop_better_history) + 1)
        ax_tests.plot(epochs, prop_better_history, 'r.', label='Proportion Better', markersize=5)
        # Plotting line separately, ignoring NaNs
        non_nan_indices = ~np.isnan(prop_better_history)
        ax_tests.plot(np.array(epochs)[non_nan_indices], np.array(prop_better_history)[non_nan_indices], 'r-')  # Connecting line for non-NaN segments

    if word_choices_history is not None:
        epochs = range(1, len(word_choices_history) + 1)
        ax_tests.plot(range(1, len(word_choices_history) + 1), word_choices_history, 'b.', markersize=5, label='Word Choices')
        # Plotting line separately, ignoring NaNs
        non_nan_indices = ~np.isnan(word_choices_history)
        ax_tests.plot(np.array(epochs)[non_nan_indices], np.array(word_choices_history)[non_nan_indices], 'b-')  # Connecting line for non-NaN segments
    # Adding grey dashed lines at y=0 and y=0.5
    ax_tests.axhline(y=0, color='mistyrose', linestyle='--')
    ax_tests.axhline(y=0.5, color='skyblue', linestyle='--')
    ax_tests.set_xlabel('Epoch')
    ax_tests.set_ylabel('Test Metrics')
    ax_tests.set_title('Test Performance Metrics')
    ax_tests.legend()

    # Plotting the heatmap for Input, Target, and Output
    ax_input_target_output_heatmap = fig.add_subplot(gs[0, 2])
    input_target_output_heatmap = ax_input_target_output_heatmap.imshow(combined_input_target_output_matrix, cmap='hot', interpolation='nearest', aspect='auto')
    ax_input_target_output_heatmap.set_title('Input, Target & Output Vectors')
    ax_input_target_output_heatmap.set_yticks([0, 1, 2])
    ax_input_target_output_heatmap.set_yticklabels(['Input', 'Target', 'Output'])
    ax_input_target_output_heatmap_cbar = fig.add_subplot(gs[0, 3])
    plt.colorbar(input_target_output_heatmap, cax=ax_input_target_output_heatmap_cbar)

    # Plot combined hidden-context heatmap
    ax_hidden_context_heatmap = fig.add_subplot(gs[1, 2])
    hidden_context_heatmap = ax_hidden_context_heatmap.imshow(combined_hidden_context_matrix, cmap='hot', interpolation='nearest', aspect='auto')
    ax_hidden_context_heatmap.set_title('Hidden & Context States')
    ax_hidden_context_heatmap.set_yticks([0, 1])
    ax_hidden_context_heatmap.set_yticklabels(['Hidden', 'Context'])
    ax_hidden_context_heatmap_cbar = fig.add_subplot(gs[1, 3])
    plt.colorbar(hidden_context_heatmap, cax=ax_hidden_context_heatmap_cbar)

    # Plot input-to-hidden weight matrix heatmap
    ax_input_hidden_heatmap = fig.add_subplot(gs[2, 2])
    input_hidden_heatmap = ax_input_hidden_heatmap.imshow(W_input_hidden, cmap='hot', interpolation='nearest', aspect='auto')
    ax_input_hidden_heatmap.set_title('Input-to-Hidden Weights')
    ax_input_hidden_heatmap_cbar = fig.add_subplot(gs[2, 3])
    plt.colorbar(input_hidden_heatmap, cax=ax_input_hidden_heatmap_cbar)

    # Plot context-to-hidden weight matrix heatmap
    ax_context_hidden_heatmap = fig.add_subplot(gs[3, 2])
    context_hidden_heatmap = ax_context_hidden_heatmap.imshow(W_context_hidden, cmap='hot', interpolation='nearest', aspect='auto')
    ax_context_hidden_heatmap.set_title('Context-to-Hidden Weights')
    ax_context_hidden_heatmap_cbar = fig.add_subplot(gs[3, 3])
    plt.colorbar(context_hidden_heatmap, cax=ax_context_hidden_heatmap_cbar)

    # Plot hidden-to-output weight matrix heatmap
    ax_hidden_output_heatmap = fig.add_subplot(gs[4, 2])
    hidden_output_heatmap = ax_hidden_output_heatmap.imshow(W_hidden_output, cmap='hot', interpolation='nearest', aspect='auto')
    ax_hidden_output_heatmap.set_title('Hidden-to-Output Weights')
    ax_hidden_output_heatmap_cbar = fig.add_subplot(gs[4, 3])
    plt.colorbar(hidden_output_heatmap, cax=ax_hidden_output_heatmap_cbar)

    plt.tight_layout()
    plt.show()





def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Function to process the lexicon and map syllables to indices
def process_lexicon(lexicon):
    unique_syllables = sorted(set(''.join(lexicon)))
    syllable_to_index = {syllable: i for i, syllable in enumerate(unique_syllables)}
    return unique_syllables, syllable_to_index

def initialize_network(lexicon, nhidden=None, method='normal', wmean=0, wsd=0.25):
    _, syllable_to_index = process_lexicon(lexicon)
    n_syllables = len(syllable_to_index)
    
    # if nhidden == None, set nhidden to by n_syllables
    nhidden = nhidden or n_syllables

    if method == "uniform":
        W_input_hidden = np.random.randn(n_syllables, nhidden) * 0.01
        W_context_hidden = np.random.randn(nhidden, nhidden) * 0.01  # New weights
        W_hidden_output = np.random.randn(nhidden, n_syllables) * 0.01
        hidden_state = np.zeros(nhidden)
        W_bias_hidden = np.random.randn(nhidden) * 0.01  # Bias weights
    else:
        W_input_hidden = np.random.normal(wmean, wsd, (n_syllables, nhidden))
        W_context_hidden = np.random.normal(wmean, wsd, (nhidden, nhidden))
        W_hidden_output = np.random.normal(wmean, wsd, (nhidden, n_syllables))
        W_bias_hidden = np.random.normal(wmean, wsd, nhidden)
        hidden_state = np.zeros(nhidden)
        

    return W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, hidden_state

def save_weights(W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, filename_prefix):
    np.save(f'{filename_prefix}_input_hidden.npy', W_input_hidden)
    np.save(f'{filename_prefix}_bias_hidden.npy', W_bias_hidden)
    np.save(f'{filename_prefix}_context_hidden.npy', W_context_hidden)
    np.save(f'{filename_prefix}_hidden_output.npy', W_hidden_output)
    print("Weights saved successfully.")
    # Example of saving weights
    # save_weights(W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, 'network_weights')


def load_weights(filename_prefix):
    W_input_hidden = np.load(f'{filename_prefix}_input_hidden.npy')
    W_bias_hidden = np.load(f'{filename_prefix}_bias_hidden.npy')
    W_context_hidden = np.load(f'{filename_prefix}_context_hidden.npy')
    W_hidden_output = np.load(f'{filename_prefix}_hidden_output.npy')
    print("Weights loaded successfully.")
    return W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output

    
#def binary_cross_entropy_loss(y_true, y_pred):
def cross_entropy_loss(y_true, y_pred):
    return -np.sum(y_true * np.log(y_pred + 1e-15) + (1 - y_true) * np.log(1 - y_pred + 1e-15))

def multi_cross_entropy_loss(y_true, y_pred):
    # Ensure small value is added to y_pred to avoid log(0)
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)

    # Calculate cross-entropy loss
    loss = -np.sum(y_true * np.log(y_pred))
    return loss

# Function to generate the sequence of words for an epoch
def generate_epoch_sequence(lexicon, frequency, lexbatch=None, no_repeat=True):
    epoch_sequence = []
    if lexbatch:
        for _ in range(lexbatch):
            batch = lexicon * frequency
            random.shuffle(batch)
            if no_repeat:
                # Ensure no repeated word at the boundary
                while batch[0] == epoch_sequence[-1]:
                    random.shuffle(batch)
            epoch_sequence.extend(batch)
    else:
        epoch_sequence = lexicon * frequency
        random.shuffle(epoch_sequence)
        if no_repeat:
            # Ensure no repeated word at the start
            while len(epoch_sequence) > 1 and epoch_sequence[0] == epoch_sequence[-1]:
                random.shuffle(epoch_sequence)

    # Convert the sequence of words into a sequence of letters
    letter_sequence = ''.join(epoch_sequence)
    return letter_sequence

# Function to encode words to input and target vectors
def encode_word(word, syllable_to_index):
    vec = np.zeros(len(syllable_to_index))
    vec[syllable_to_index[word]] = 1
    # vec[syllable_to_index[word[-1]]] = 1  # Encoding the last syllable as target
    return vec

# Function to calculate loss on a given sequence of words
def calculate_loss_on_sequence(sequence, W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, \
                               syllable_to_index, hidden_state):
    total_loss = 0
    num_syllables = len(sequence) - 1  # Subtract 1 because the last syllable has no following syllable as a target

    for i in range(len(sequence) - 1):
        input_vec = encode_word(sequence[i], syllable_to_index)
        target_vec = encode_word(sequence[i + 1], syllable_to_index)
        _, output_vec = forward_pass(input_vec, W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, hidden_state)
        this_loss = cross_entropy_loss(target_vec, output_vec)
        total_loss += this_loss
        # print(f'\n#{sequence}\n\tINPUT {input_vec}\n\tTARGET {target_vec}\n\tOUTPUT {output_vec}\n\tthisLOSS {this_loss}, totLOSS {total_loss}')
    
    # time.sleep(4)
    average_loss_per_syllable = total_loss / num_syllables
    return average_loss_per_syllable
    # return total_loss

# Function to calculate error for individual word for all but last item, since target is not defined
def calculate_word_error(word, W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, \
                         syllable_to_index, hidden_state):
    total_error = 0
    for i in range(len(word) - 1):
        input_vec = encode_word(word[i], syllable_to_index)
        target_vec = encode_word(word[i + 1], syllable_to_index)
        _, output_vec = forward_pass(input_vec, \
                                     W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, hidden_state)
        total_error += cross_entropy_loss(target_vec, output_vec)
    return total_error

# Function to calculate word choices
def calculate_word_choices(lex_errors, comp_errors):
    word_better_count = 0
    total_comparisons = 0
    for lex_error, comp_error in zip(lex_errors, comp_errors):
        total_comparisons += 1
        if lex_error < comp_error:
            word_better_count += 1
    return word_better_count / total_comparisons if total_comparisons > 0 else 0


# Function to calculate proportion better
def prop_better_and_word_choices(lexicon, comparison_items, \
                                W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, \
                                syllable_to_index):
    # Calculate errors for each word in lexicon and comparison items
    lex_errors = [calculate_word_error(word, W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, \
                                       syllable_to_index, np.zeros_like(hidden_state)) for word in lexicon]
    comp_errors = [calculate_word_error(word, W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, \
                                        syllable_to_index, np.zeros_like(hidden_state)) for word in comparison_items]

    # Word choices calculation
    wchoices = calculate_word_choices(lex_errors, comp_errors)
    
    # Mean errors
    lexicon_mean_error = sum(lex_errors) / len(lex_errors)
    comparison_mean_error = sum(comp_errors) / len(comp_errors)

    # Proportion better calculation
    pbetter = (comparison_mean_error - lexicon_mean_error) / (comparison_mean_error + lexicon_mean_error)
    
    return pbetter, wchoices

# The forward pass of the network with context-to-hidden connections
def forward_pass(input_vec, W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, hidden_state):
    # new_hidden_state = np.tanh(np.dot(input_vec, W_input_hidden) + np.dot(hidden_state, W_context_hidden))
    new_hidden_state = np.tanh(np.dot(input_vec, W_input_hidden) + W_bias_hidden + np.dot(hidden_state, W_context_hidden))
    # new_hidden_state = np.tanh(np.dot(input_vec, W_input_hidden) + W_bias_hidden) #+ np.dot(hidden_state, W_context_hidden))
    output_vec = sigmoid(np.dot(new_hidden_state, W_hidden_output))  # Applying sigmoid activation
    return new_hidden_state, output_vec


# The training function
def train_network(lexicon, epochs, W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, 
                  hidden_state, learning_rate=0.01, print_interval=10, test_interval=100,
                  colorbar_width=0.02, colorbar_pad=0.02, nhidden=None, testlexicon=None):

    if testlexicon is None:
        testlexicon = lexicon
    
    unique_syllables, syllable_to_index = process_lexicon(lexicon)
    
    if nhidden is None:
        nhidden = len(unique_syllables)

    # initialize history vectors
    input_history = []  # To store input vectors for visualization
    output_history = []  # To store output vectors for visualization
    hidden_state_history = []  # To store hidden state vectors for visualization
    loss_history = []
    loss_lexicon_history = [np.nan] * epochs  # Initialize with NaNs
    prop_better_history = [np.nan] * epochs  # Initialize with NaNs
    word_choices_history = [np.nan] * epochs  # Initialize with NaNs
    
    for epoch in range(epochs):
        letter_sequence = generate_epoch_sequence(lexicon, 1)
        # epoch_sequence = generate_epoch_sequence(lexicon, 1)  # Frequency set to 1 for simplicity
        total_loss = 0

        # Generate input and target sequences
        input_sequence = letter_sequence[:-1]  # All except the last element
        target_sequence = letter_sequence[1:]  # All except the first element
        
        combined_input_target_matrix = np.zeros((2, len(syllable_to_index)))
        combined_input_target_output_matrix = np.zeros((3, len(syllable_to_index)))  # Adding a row for output

        combined_hidden_context_matrix = np.zeros((2, nhidden))
        # break
        
        # Backpropagation (updated for Cross-Entropy Loss)
        for input_letter, target_letter in zip(input_sequence, target_sequence):
            input_vec = encode_word(input_letter, syllable_to_index)
            target_vec = encode_word(target_letter, syllable_to_index)
        # for input_word, target_word in zip(input_sequence, target_sequence):
        #     input_vec = encode_word(input_word, syllable_to_index)
        #     target_vec = encode_word(target_word, syllable_to_index)

            new_hidden_state, output_vec = forward_pass(input_vec, W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, hidden_state)

            #nhidden = len(hidden_state)

            
            # Calculate loss
            loss = cross_entropy_loss(target_vec, output_vec)
            loss = cross_entropy_loss(target_vec, output_vec)
            total_loss += loss

            # Error signal for output layer
            error_signal = output_vec - target_vec

            # Gradient for W_hidden_output
            dW_hidden_output = np.outer(new_hidden_state, error_signal)

            # Error signal for hidden layer (propagating back through the network)
            hidden_error_signal = np.dot(W_hidden_output, error_signal) * (1 - new_hidden_state ** 2)

            # Gradient for W_context_hidden
            dW_context_hidden = np.outer(hidden_state, hidden_error_signal)

            # Gradient for W_input_hidden
            dW_input_hidden = np.outer(input_vec, hidden_error_signal)

            # Gradient for W_bias_hidden
            dW_bias_hidden = hidden_error_signal

            # Update weights
            W_input_hidden -= learning_rate * dW_input_hidden
            W_context_hidden -= learning_rate * dW_context_hidden
            W_hidden_output -= learning_rate * dW_hidden_output
            W_bias_hidden -= learning_rate * dW_bias_hidden


            context_state = hidden_state
            hidden_state = new_hidden_state
            hidden_state_history.append(hidden_state.copy())  # Store the hidden state
            
            combined_input_target_output_matrix[0, :] = input_vec
            combined_input_target_output_matrix[1, :] = target_vec
            combined_input_target_output_matrix[2, :] = output_vec  # Add output vector to the matrix

            combined_hidden_context_matrix[0, :] = hidden_state
            combined_hidden_context_matrix[1, :] = context_state


        # At the end of each epoch, append the average loss to loss_history
        average_loss = total_loss / len(input_sequence)
        loss_history.append(average_loss)


        # # print(f"Epoch {epoch+1}, Total loss: {total_loss}"
        # loss_history.append(total_loss / len(input_sequence))

        input_history.append(input_vec)
        output_history.append(output_vec)

        if epoch % test_interval == 0:
            # Test (1): Loss on the Lexicon
            loss_lexicon = calculate_loss_on_sequence(''.join(testlexicon), W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, syllable_to_index, hidden_state)
            loss_lexicon_history[epoch] = loss_lexicon
            shifted_lexicon = testlexicon[1:] + testlexicon[:1]  # Shift the lexicon order
            loss_shifted = calculate_loss_on_sequence(''.join(shifted_lexicon), W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, syllable_to_index, hidden_state)

            # Test (2): Proportion Better & word choices
            proportion_better, word_choices = prop_better_and_word_choices(lexicon, comparison_items, \
                                                                           W_input_hidden, W_bias_hidden, \
                                                                           W_context_hidden, W_hidden_output, \
                                                                           syllable_to_index)
            prop_better_history[epoch] = proportion_better
            word_choices_history[epoch] = word_choices

            # print(f"\nEpoch {epoch}: Loss Lexicon {loss_lexicon:.4f}, Loss Shifted {loss_shifted:.4f}, Proportion Better {proportion_better:.4f}, Word Choices {word_choices:.4f}")
            print(f"\nEpoch {epoch}: Test loss {loss_lexicon:.4f}, Proportion Better {proportion_better:.4f}, Word Choices {word_choices:.4f}")
            # if epoch > 4950:
            #     time.sleep(1)

        if epoch % print_interval == 0 or epoch == epochs - 1:
            plot_training_progress(loss_history, word_choices_history, prop_better_history, combined_input_target_output_matrix, combined_hidden_context_matrix, 
                                   W_input_hidden, W_context_hidden, W_hidden_output, epoch, loss_lexicon_history)
            print(f'\rInput: {input_sequence}\tTarget: {target_sequence}        ',end="")
            

        hidden_state_history = []  # Reset hidden state history for next interval
        print(f"\nEpoch {epoch}: Test loss {loss_lexicon:.4f}, Proportion Better {proportion_better:.4f}, Word Choices {word_choices:.4f}")
        # print(f"\nEpoch {epoch}: Loss Lexicon {loss_lexicon:.4f}, Loss Shifted {loss_shifted:.4f}, Proportion Better {proportion_better:.4f}, Word Choices {word_choices:.4f}")
        
    return W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, hidden_state





# Example usage
# Here are 4 'words' akin to the items used by Saffran et al. (1996)
lexicon = ['ABC', 'DEF', 'GHI', 'JKL']
# testlexicon = ['ABC', 'DEF'] # Saffran et al only used the first 2 words
testlexicon=None

# Here are 4 'part-words' made up of the last syllable of 1 word and the first 2 syllables of another
comparison_items = ['CDE', 'FGH', 'IJK', 'LAB']  # Example
# comparison_items = ['IJK', 'AGH']  # Modeled directly after the items used by Saffran et al
# if nhid is set to None, the actual number will be the same as number of unique syllables in lexicon,
# which is the approach of French et al. 2011
#nhid = 2
nhid = 6
#nhid = 20
# nhid = 12
# nhid = None 
W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, hidden_state = initialize_network(lexicon, \
                                                                                                    nhidden=nhid)
nepochs = 2000
W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, hidden_state = \
    train_network(lexicon, nepochs, W_input_hidden, W_bias_hidden, W_context_hidden, W_hidden_output, \
                  hidden_state, print_interval=10, nhidden=nhid, learning_rate=0.005, test_interval=10, \
                  testlexicon=testlexicon)









