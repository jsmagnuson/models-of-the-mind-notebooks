








import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from ipywidgets import interact

# Sample data
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])  # Feature set
y = np.array([6, 8, 9, 11])                     # Target values

# Mean Squared Error function
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# Model prediction
def model_prediction(X, w1, w2):
    return w1 * X[:, 0] + w2 * X[:, 1]

# Generating a range of weight values
w1 = np.linspace(-10, 10, 400)
w2 = np.linspace(-10, 10, 400)

# Creating a meshgrid for the weights
W1, W2 = np.meshgrid(w1, w2)

# Calculating the error for each combination of weights
errors = np.array([mse(y, model_prediction(X, w1, w2)) for w1, w2 in zip(np.ravel(W1), np.ravel(W2))])
Z = errors.reshape(W1.shape)

def plot_surface(elev=30, azim=30):
    fig = plt.figure(figsize=(10, 5))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(W1, W2, Z, cmap='viridis', edgecolor='none')
    ax.set_xlabel('Weight 1')
    ax.set_ylabel('Weight 2')
    ax.set_zlabel('Error')
    ax.set_title('Error Surface')
    ax.view_init(elev=elev, azim=azim)
    plt.show()

interact(plot_surface, elev=(0, 90), azim=(0, 90))









import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from ipywidgets import interact

# Sample data
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])  # Feature set
y = np.array([6, 8, 9, 11])                     # Target values

# Modified Mean Squared Error function to introduce stronger local minima
def mse_with_minima(y_true, y_pred, w1, w2):
    base_mse = np.mean((y_true - y_pred) ** 2)
    # Amplify the oscillations to create more distinct local minima
    local_minima = 50 * (np.sin(0.9 * w1) * np.sin(0.3 * w2) + 0.5 * np.sin(0.9 * w1) * np.cos(0.5 * w2))
    return base_mse + local_minima

# Model prediction
def model_prediction(X, w1, w2):
    return w1 * X[:, 0] + w2 * X[:, 1]

# Generating a range of weight values
w1 = np.linspace(-10, 10, 400)
w2 = np.linspace(-10, 10, 400)

# Creating a meshgrid for the weights
W1, W2 = np.meshgrid(w1, w2)

# Calculating the error for each combination of weights
errors = np.array([mse_with_minima(y, model_prediction(X, w1, w2), w1, w2) 
                   for w1, w2 in zip(np.ravel(W1), np.ravel(W2))])
Z = errors.reshape(W1.shape)

def plot_surface(elev=30, azim=30):
    fig = plt.figure(figsize=(15, 15))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(W1, W2, Z, cmap='viridis', edgecolor='none')
    ax.set_xlabel('Weight 1')
    ax.set_ylabel('Weight 2')
    ax.set_zlabel('Error')
    ax.set_title('Error Surface with Local Minima')
    ax.view_init(elev=elev, azim=azim)
    plt.show()

interact(plot_surface, elev=(0, 90), azim=(0, 90))









import numpy as np
import matplotlib.pyplot as plt

# Define the function and its derivative
def f(x):
    return x ** 2

def df(x):
    return 2 * x

# Define the point at which we calculate the derivative
x0 = 2
y0 = f(x0)
slope = df(x0)

# Define the tangent line at x0
def tangent_line(x):
    return slope * (x - x0) + y0

# Generate x values
x = np.linspace(-1, 3, 400)

# Plot the function
plt.figure(figsize=(8, 4))
plt.plot(x, f(x), label="f(x) = x^2", color="blue")

# Plot the tangent line
x_tangent = np.linspace(1, 3, 10)
plt.plot(x_tangent, tangent_line(x_tangent), label="Tangent at x = 2", color="red", linestyle="--")

# Highlight the point of tangency
plt.scatter([x0], [y0], color="black")
plt.text(x0, y0, f"  ({x0}, {y0})", verticalalignment='bottom')

# Add labels and title
plt.xlabel("x")
plt.ylabel("f(x)")
plt.title("Illustration of Instantaneous Rate of Change")
plt.legend()
plt.grid(True)
plt.show()


















def make_activation_plots():

    import numpy as np
    import matplotlib.pyplot as plt

    # Activation functions
    def linear_activation(x):
        return x

    def step_activation(x, thresh=0.5):
        return np.where(x > thresh, 1, 0)

    def sigmoid_activation(x):
        return 1 / (1 + np.exp(-x))

    def tanh_activation(x):
        return np.tanh(x)

    def relu_activation(x):
        return np.maximum(0, x)

    # Generating values from -10 to 10
    x = np.linspace(-6, 6, 100)

    # Plotting
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    # Function to add annotations
    def add_annotation(ax, formula, xpos=0.5, ypos=0.15, fontsize=16, alpha=0.8):
        ax.text(xpos, ypos, formula, horizontalalignment='center', 
                verticalalignment='center', transform=ax.transAxes, 
                fontsize=fontsize, bbox=dict(facecolor='white', alpha=alpha))

    # Function to add grey dashed lines
    def add_zero_lines(ax):
        ax.axhline(y=0, color='lightgrey', linestyle='--', linewidth=1)
        ax.axvline(x=0, color='lightgrey', linestyle='--', linewidth=1)

    # Linear
    axes[0,0].plot(x, linear_activation(x))
    axes[0,0].set_title('Linear Activation')
    add_annotation(axes[0,0], 'f(x) = x', 0.75)

    # Step
    axes[0,1].plot(x, step_activation(x, 0.5))
    axes[0,1].set_title('Step Activation (0.5)')
    add_annotation(axes[0,1], 'f(x) = 1 if x > 0.5\n       else 0', 0.8, 0.2, alpha=1, fontsize=16)

    # Sigmoid
    axes[0,2].plot(x, sigmoid_activation(x))
    axes[0,2].set_title('Sigmoid Activation')
    #add_annotation(axes[0,2], 'f(x) = 1 / (1 + exp(-x))', 0.65, 0.15)
    add_annotation(axes[0,2], 'f(x) = $\\frac{1}{1 + e^{-x}}$', 0.75, fontsize=20)

    # ReLU
    axes[1,0].plot(x, relu_activation(x))
    axes[1,0].set_title('ReLU Activation')
    add_annotation(axes[1,0], 'f(x) = max(0, x)', 0.3, 0.4)

    # Step
    axes[1,1].plot(x, step_activation(x, -1.5))
    axes[1,1].set_title('Step Activation (-1.5)')
    add_annotation(axes[1,1], 'f(x) = 1 if x > -1.5\n       else 0', 0.8, 0.2, fontsize=16, alpha=1)

    # Tanh
    axes[1,2].plot(x, tanh_activation(x))
    axes[1,2].set_title('Tanh Activation')
    add_annotation(axes[1,2], 'f(x) = $\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$', 0.75, fontsize=20)

    for ax_x in range(2):
        for ax_y in range(3):
            add_zero_lines(axes[ax_x,ax_y])


    # Adjusting layout
    plt.tight_layout()
    plt.show()






make_activation_plots()





import numpy as np
import matplotlib.pyplot as plt
from IPython.display import display, clear_output
import time 

# Activation functions and their derivatives

# sigmoid: translates - and + values to range 0,1; 
#         -6 --> 0.002
#         -4 --> 0.02
#         -2 --> 0.12
#          0 --> 0.50
#          2 --> 0.88
#          4 --> 0.98
#          6 --> 0.998
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# sigmoid: translates - and + values to range -1,1; 
#         -6 --> -0.9999...
#         -4 --> -0.9993...
#         -2 --> -0.964
#         -1 --> -0.762
#       -0.5 --> -0.462
#          0 -->  0.000
#        0.5 -->  0.462
#          1 -->  0.762
#          2 -->  0.964
#          4 -->  0.9993...
#          6 -->  0.99999..
def tanh(x):
    return np.tanh(x)

# need the derivatives for backpropogation 
def sigmoid_derivative(x):
    return x * (1 - x)

def tanh_derivative(x):
    return 1 - np.tanh(x)**2

# Mean Squared Error loss
def mse_loss(y_true, y_pred):
    return ((y_true - y_pred) ** 2).mean()

# training function
def train_xor_model(inputs, output, epochs, lr_initial=0.5, lr_reduce=None, plot_interval=50, 
                    show_wtval=True, rseed=None, wtsd=0.5,
                   weight_init_min=None, weight_init_max=None, stop_criterion=0.2, tries=None):
    '''
        Function to train the XOR network. 
    '''
        
    # User can set a random seed for reproducibility
    np.random.seed(rseed)
    
    # initialize weights in range specified by weight_init_min and _max
    
    num_inputs = 2
    num_hidden = 2
    num_output = 1
    num_bias_hid = 1
    num_bias_out = 1
    
    # you can go back to the old way by specifying weight_init_min and weight_init_max values, 
    # but default is to use wtsd (see else)
    if weight_init_min is not None and weight_init_max is not None: 
        # 2 input and 2 hidden nodes, hence weights_input_hidden has shape 2, 2
        weights_input_hidden = np.random.uniform(weight_init_min, weight_init_max, (num_inputs, num_hidden))
        # 2 hidden and 1 output nodes, hence weights_hidden_output has shape 2, 1
        weights_hidden_output = np.random.uniform(weight_init_min, weight_init_max, (num_hidden, num_output))
        # 1 bias to 2 hidden has shape 1, 2
        bias_hidden = np.zeros((num_bias_hid, num_hidden))
        # 1 bias to 1 output has shape 1, 1
        bias_output = np.zeros((num_bias_out, num_output))
    else:
        # initialize weights
        weights_input_hidden = np.random.normal(0, wtsd, (num_inputs, num_hidden))
        weights_hidden_output = np.random.normal(0, wtsd, (num_hidden, num_output))
        bias_hidden = np.random.normal(0, wtsd, (num_bias_hid, num_hidden))
        bias_output = np.random.normal(0, wtsd, (num_bias_out, num_output))
    
    losses = [] # list to store loss (error) values
    lr = lr_initial  # Initialize learning rate
    lr_values = []
    

    def calculate_reverse_sigmoid_lr(lr_initial, epoch, epochs, lr_reduce=0.1, k=10):
        """
        Calculate the learning rate following a reverse sigmoid curve.

        :param lr_initial: Initial learning rate.
        :param epoch: Current epoch number.
        :param epochs: Total number of epochs.
        :param final_lr_fraction: Fraction of the initial learning rate at the final epoch.
        :param k: Steepness of the sigmoid curve.
        :return: Adjusted learning rate.
        """
        # Scale the epoch number to [0, 1]
        
        min_lr = lr_reduce * lr_initial
        red_lr = lr_initial - min_lr
        proportion_epoch = epoch + 1 / epochs + 1
        scaled_epoch = proportion_epoch * red_lr
        

        # Apply the reverse sigmoid function
        lr = lr_initial * (lr_reduce + (1 - lr_reduce) * (1 - 1 / (1 + np.exp(-k * (proportion_epoch - 0.5)))))

        return lr
    
    for epoch in range(epochs):
        # Forward pass
        
        # input to the hidden nodes is the dot product of the input values and
        # the weights_input_hidden matrix plus the value of the bias weight 
        # (since bias input is always 1)
        hidden_input = inputs.dot(weights_input_hidden) + bias_hidden
        hidden_output = tanh(hidden_input) # transform using tanh function
        
        # hidden to output is dot product of hidden values and 
        # weights_hidden_output plus the value of the bias weight 
        # (since bias input is always 1)
        output_input = hidden_output.dot(weights_hidden_output) + bias_output
        final_output = sigmoid(output_input) # transform using sigmoid function

        # Calculate loss -- MSE of expected output and observed output
        loss = mse_loss(output, final_output)
        losses.append(loss) # append to list
        
        # Adjust learning rate based on loss (adaptive learning rate)
        if lr_reduce is not None:
            #lr = lr_initial * (lr_reduce * loss) - (0.01 * (1/(epoch + 1)))
            #lr -= lr * (lr_reduce)
            # lr *= 1 - (lr_reduce *  ((epoch + 1) / (epochs + 1)))
            #lr = calculate_exponential_lr(lr_initial, epoch, epochs)
            lr = calculate_reverse_sigmoid_lr(lr_initial, epoch, epochs)
        lr_values.append(lr)  # Store current learning rate
        
        # Backward pass: compute gradients and update weights and biases
        error = output - final_output # raw difference
        d_final_output = error * sigmoid_derivative(final_output) # Loss derivative wrt final output
        error_hidden = d_final_output.dot(weights_hidden_output.T) # Error propagated to hidden layer
        d_hidden_output = error_hidden * tanh_derivative(hidden_output) # Loss deriv wrt hidden output

        # Update weights and biases using gradient descent
        # d_ objects are the loss derivatives; lr is learning rate
        weights_hidden_output += hidden_output.T.dot(d_final_output) * lr
        bias_output += np.sum(d_final_output, axis=0, keepdims=True) * lr
        weights_input_hidden += inputs.T.dot(d_hidden_output) * lr
        bias_hidden += np.sum(d_hidden_output, axis=0, keepdims=True) * lr

        # Make plots if epoch modulus plot_interval is plot_interval - 1 -- gives us 
        # epochs that are at the interval plus 1
        if epoch % plot_interval == plot_interval-1:
            plot_training_results(weights_input_hidden, bias_hidden, weights_hidden_output, \
                                  bias_output, losses, lr_values, inputs, output, epoch, show_wtval=show_wtval, tries=tries, rseed=rseed)

        # Check stopping criterion (optional)
        if stop_criterion is not None:
            if ((final_output[output == 1] > (1-stop_criterion)).all() and (final_output[output == 0] < stop_criterion).all()):
                plot_training_results(weights_input_hidden, bias_hidden, weights_hidden_output, \
                                     bias_output, losses, lr_values, inputs, output, epoch, show_wtval=show_wtval, tries=tries, rseed=rseed)
                # print(f"Stopping criterion met at epoch {epoch+1}")
                break

    return weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, losses, epoch



def plot_training_results(weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, 
                          losses, lr_values, X, Y, epoch, show_wtval=True, tries=None, rseed=None):
    clear_output(wait=True)
    fig, axs = plt.subplots(2, 3, figsize=(15, 8))  # Increase subplot grid size

    # Loss plot
    axs[0, 0].plot(losses)
    if tries is not None:
        titletext = f'Try {tries}, Epoch {epoch+1}'
    else: 
        titletext = f'Epoch {epoch+1}'
    if rseed is not None:
        titletext += f", rseed {rseed}"
        
    axs[0, 0].set_title(titletext)
    axs[0, 0].set_xlabel('Epoch')
    axs[0, 0].set_ylabel('Loss')
    
    # Decision boundary plot
    xx, yy = np.meshgrid(np.linspace(-0.1, 1.1, 100), np.linspace(-0.1, 1.1, 100))
    grid = np.c_[xx.ravel(), yy.ravel()]
    grid_hidden = tanh(grid.dot(weights_input_hidden) + bias_hidden)
    grid_final = sigmoid(grid_hidden.dot(weights_hidden_output) + bias_output)
    zz = grid_final.reshape(xx.shape)
    axs[0, 2].contourf(xx, yy, zz, levels=[-1, 0.5, 1], colors=['pink', 'blue'])
    axs[0, 2].scatter(inputs[:, 0], inputs[:, 1], c=output.reshape(-1), edgecolor='k')
    axs[0, 2].set_title('Decision Boundary')

    # Weight heatmaps including biases
    weights_input_hidden_with_bias = np.vstack([weights_input_hidden, bias_hidden]).T
    weights_hidden_output_with_bias = np.vstack([weights_hidden_output, bias_output]).T

    # Finding the maximum and minimum values
    min_weight = -2
    max_weight = 2
    min_weight = min(weights_input_hidden_with_bias.min(), weights_hidden_output_with_bias.min(), min_weight)
    max_weight = max(weights_input_hidden_with_bias.max(), weights_hidden_output_with_bias.max(), max_weight)

    im1 = axs[1, 0].imshow(weights_input_hidden_with_bias, cmap='coolwarm', interpolation='nearest', vmin=min_weight, vmax=max_weight)
    axs[1, 0].set_title("Input-Hidden Weights Heatmap")
    if show_wtval:
        for i in range(weights_input_hidden_with_bias.shape[0]):
            for j in range(weights_input_hidden_with_bias.shape[1]):
                text = axs[1, 0].text(j, i, f'{weights_input_hidden_with_bias[i, j]:.3f}', ha="center", va="center", color="yellow", fontsize=15)
    axs[1, 0].set_ylabel("Hidden nodes")
    axs[1, 0].set_xlabel("Inputs + Bias")
    axs[1, 0].set_xticks(np.arange(3))
    axs[1, 0].set_yticks(np.arange(2))

    im2 = axs[1, 1].imshow(weights_hidden_output_with_bias, cmap='coolwarm', interpolation='nearest', vmin=min_weight, vmax=max_weight)
    axs[1, 1].set_title("Hidden-Output Weights Heatmap")
    if show_wtval:
        for i in range(weights_hidden_output_with_bias.shape[0]):
            for j in range(weights_hidden_output_with_bias.shape[1]):
                text = axs[1, 1].text(j, i, f'{weights_hidden_output_with_bias[i, j]:.3f}', ha="center", va="center", color="yellow", fontsize=15)
    axs[1, 1].set_xlabel("Hidden nodes + Bias")
    axs[1, 1].set_ylabel("Output")
    axs[1, 1].set_yticks(np.arange(1))
    axs[1, 1].set_xticks(np.arange(3))

    fig.colorbar(im1, ax=axs[1, 0], orientation='vertical', shrink=0.9)
    fig.colorbar(im2, ax=axs[1, 1], orientation='vertical', shrink=0.9)

    # Learning rate plot
    axs[0, 1].plot(lr_values)  # Plot learning rate values
    axs[0, 1].set_title("Learning Rate Over Epochs")
    axs[0, 1].set_xlabel("Epoch")
    axs[0, 1].set_ylabel("Learning Rate")

    # hide the unused bottom right panel
    axs[1, 2].axis('off')

    display(fig)

    plt.ioff()






# XOR Data
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
output = np.array([[0], [1], [1], [0]])

# Training parameters
# epochs = 300000
epochs = 8000
lr = 0.01
rseed = None
# rseed = 1

# Training
max_tries = 3
tries = 0
last_epoch = epochs
while last_epoch >= epochs - 1 and tries < max_tries:
    if tries > 0 and rseed is not None:
        # change the random seed or it will just repeat the same simulation over and over
        rseed += tries
        
    tries += 1
    weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, losses, \
        last_epoch = train_xor_model(inputs, output, epochs, lr_reduce=None, \
                                     lr_initial=0.05, plot_interval=100, tries=tries, rseed=rseed, stop_criterion=0.2)

    if last_epoch == (epochs - 1):
        if tries < max_tries:
            print(f'### Try {tries}: Failed to meet convergence criteria by epoch {last_epoch + 1}. RESTARTING.')
        else:
            print(f'### Try {tries}: Failed to meet convergence criteria by epoch {last_epoch + 1}. GIVING UP.')
        # Test predictions
        for i in range(len(inputs)):
            hidden_input = inputs[i].dot(weights_input_hidden) + bias_hidden
            hidden_output = tanh(hidden_input)
            final_input = hidden_output.dot(weights_hidden_output) + bias_output
            final_output = sigmoid(final_input)
            print(f"# input: {inputs[i]}, predicted output: {final_output[0][0]:.4f}")
        time.sleep(4)
    else:
        print(f'### Try {tries}: Met convergence criteria at epoch {last_epoch + 1}.')
        
# # Test predictions
# hidden_states = []
# for i in range(len(inputs)):
#     hidden_input = inputs[i].dot(weights_input_hidden) + bias_hidden
#     hidden_output = tanh(hidden_input)
#     hidden_states.append(hidden_output)
#     final_input = hidden_output.dot(weights_hidden_output.flatten()) + bias_output
#     final_output = sigmoid(final_input)
#     # print(f"Input: {inputs[i]}, Predicted Output: {final_output[0][0]:.4f}")
#     print(f"Input: {inputs[i]}, Hidden States: {hidden_output}, Predicted Output: {final_output[0][0]:.4f}")
# # Convert hidden_states to a 2x4 matrix
# # Convert hidden_states to a 4x2 matrix
# hidden_states_matrix = np.array(hidden_states)

# Test predictions
hidden_states = []
hid_inputs = []
for i in range(len(inputs)):
    hidden_input = inputs[i].dot(weights_input_hidden) + bias_hidden
    hid_inputs.append(hidden_input.flatten())
    hidden_output = tanh(hidden_input)
    # Flatten the hidden_output to a 1D array and append
    hidden_states.append(hidden_output.flatten())
    final_input = hidden_output.dot(weights_hidden_output) + bias_output
    final_output = sigmoid(final_input)
    print(f"Input: {inputs[i]}, Hidden States: {hidden_output.flatten()}, Predicted Output: {final_output[0][0]:.4f}")

# Convert hidden_states to a 4x2 matrix
hidden_states_matrix = np.array(hidden_states)
hidden_inputs_matrix = np.array(hid_inputs)

print(hidden_states_matrix)

plt.close('all')








import numpy as np
import matplotlib.pyplot as plt

def plot_network_architecture_with_all_weights_and_biases(weights_input_hidden, 
                                                          bias_hidden, weights_hidden_output, bias_output,
                                                         fsize=12, lw=1):
    fig, ax = plt.subplots(figsize=(9, 4))

    
    # Normalize weights for alpha values
    fc1_weights_normalized = np.abs(weights_input_hidden) / np.max(np.abs(weights_input_hidden))
    fc2_weights_normalized = np.abs(weights_hidden_output) / np.max(np.abs(weights_hidden_output))

    # Neuron positions with aligned bias nodes
    input_neurons = [(1, 3), (1, 1)]
    hidden_neurons = [(2, 3), (2, 1)]
    output_neurons = [(3, 2)]
    bias_neurons = [(1, 2), (2, 2)]  # Aligned bias nodes with input and hidden nodes

    # Plot neurons
    for neuron in input_neurons + hidden_neurons + output_neurons:
        ax.plot(neuron[0], neuron[1], 'o', color='blue' if neuron in input_neurons else 'green' if neuron in hidden_neurons else 'red')

    # Connect input to hidden layer with weights
    for i, input_neuron in enumerate(input_neurons):
        for j, hidden_neuron in enumerate(hidden_neurons):
            weight = weights_input_hidden[i, j]
            alpha = fc1_weights_normalized[i, j]
            ax.annotate('', xy=hidden_neuron, xytext=input_neuron, arrowprops=dict(arrowstyle="->", lw=lw, color='gray', alpha=alpha))
            text_pos = np.array(input_neuron) + 0.2 * (np.array(hidden_neuron) - np.array(input_neuron))
            ax.text(*text_pos, f'{weight:.2f}', color='black', fontsize=fsize)

    # Connect hidden to output layer with weights
    for i, hidden_neuron in enumerate(hidden_neurons):
        weight = weights_hidden_output[i, 0]
        alpha = fc2_weights_normalized[i, 0]
        ax.annotate('', xy=output_neurons[0], xytext=hidden_neuron, arrowprops=dict(arrowstyle="->", lw=lw, color='gray', alpha=alpha))
        text_pos = np.array(hidden_neuron) + 0.2 * (np.array(output_neurons[0]) - np.array(hidden_neuron))
        ax.text(*text_pos, f'{weight:.2f}', color='black', fontsize=fsize)

    # Plot and connect bias nodes for hidden layer, include weights for bias to hidden neurons
    bias_neuron = bias_neurons[0]
    ax.plot(*bias_neuron, 'o', color='red')
    for i, hidden_neuron in enumerate(hidden_neurons):
        ax.annotate('', xy=hidden_neuron, xytext=bias_neuron, arrowprops=dict(arrowstyle="->", lw=lw, color='red', alpha=0.7))
        text_pos = np.array(bias_neuron) + 0.2 * (np.array(hidden_neuron) - np.array(bias_neuron))
        ax.text(*text_pos, f'{bias_hidden[0, i]:.2f}', color='red', fontsize=fsize)

    # Plot and connect bias node for output layer
    output_bias = bias_neurons[-1]
    ax.plot(*output_bias, 'o', color='red')
    ax.annotate('', xy=output_neurons[0], xytext=output_bias, arrowprops=dict(arrowstyle="->", lw=lw, color='red', alpha=0.7))
    text_pos = np.array(output_bias) + 0.2 * (np.array(output_neurons[0]) - np.array(output_bias))
    ax.text(*text_pos, f'{bias_output[0, 0]:.2f}', color='red', fontsize=fsize)

    ax.axis('off')
    plt.title("XOR network (bias nodes & connections in red)")

    # Print the weight matrices and bias vectors
    print("First Layer Weights:\n", weights_input_hidden)
    print("First Layer Biases:\n", bias_hidden)
    print("Second Layer Weights:\n", weights_hidden_output)
    print("Second Layer Biases:\n", bias_output)
    
    return fig

plt.close('all')
# To use the function, pass your network's weights and biases
network_plot = plot_network_architecture_with_all_weights_and_biases(weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)
plt.show()









import matplotlib.pyplot as plt
import numpy as np

def plot_hidden_states_and_inputs(inputs, hidden_states, hidden_inputs, outputs, ptitle='Hidden Unit States', jitter=0.0):
    """
    Plots the input values, raw hidden inputs, and the hidden unit states for given inputs and outputs.

    Parameters:
    inputs (np.array): Array of input points.
    hidden_states (np.array): Array of hidden states corresponding to each input point.
    hidden_inputs (np.array): Array of raw hidden inputs corresponding to each input point.
    outputs (np.array): Array of output values corresponding to each input point.
    jitter (float): Amount of jitter to apply to the points.
    """
    fig, axes = plt.subplots(1, 3, figsize=(24, 6))  # Three panels side by side

    # Plotting the input values
    ax = axes[0]
    for point, output in zip(inputs, outputs):
        color = 'blue' if output == 1 else 'red'
        marker = 'o' if output == 1 else 'x'
        ax.scatter(point[0], point[1], color=color, marker=marker, s=100)

    ax.set_title('Input Values')
    ax.set_xlabel('Input 1')
    ax.set_ylabel('Input 2')
    ax.grid(True, color='lightgrey')

    # Plotting the raw hidden inputs
    ax = axes[1]
    for point, hidden_input, output in zip(inputs, hidden_inputs, outputs):
        color = 'blue' if output == 1 else 'red'
        marker = 'o' if output == 1 else 'x'
        jittered_state = hidden_input + np.random.normal(0, jitter, hidden_input.shape)
        ax.scatter(jittered_state[0], jittered_state[1], color=color, marker=marker, s=100)

    ax.set_title('Raw Hidden Inputs')
    ax.set_xlabel('Hidden Node 1 Input')
    ax.set_ylabel('Hidden Node 2 Input')
    ax.grid(True, color='lightgrey')

    # Plotting the activated hidden states
    ax = axes[2]
    for point, hidden_state, output in zip(inputs, hidden_states, outputs):
        color = 'blue' if output == 1 else 'red'
        marker = 'o' if output == 1 else 'x'
        jittered_state = hidden_state + np.random.normal(0, jitter, hidden_state.shape)
        ax.scatter(jittered_state[0], jittered_state[1], color=color, marker=marker, s=100)

    ax.set_title(ptitle)
    ax.set_xlabel('Hidden Node 1 Activation')
    ax.set_ylabel('Hidden Node 2 Activation')
    ax.grid(True, color='lightgrey')

    
    plt.rcParams['font.size'] = 12  # Default font size for all text

    plt.show()

# Example usage
plot_hidden_states_and_inputs(inputs, hidden_states_matrix, hidden_inputs_matrix, output, ptitle='Hidden Unit States for XOR', jitter=0.009)







