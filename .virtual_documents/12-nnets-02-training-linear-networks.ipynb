








import numpy as np
import matplotlib.pyplot as plt
from IPython.display import clear_output
import time
from matplotlib.ticker import MaxNLocator
from matplotlib.colors import ListedColormap

def train_perceptron(inputs, expected_outputs, learning_rate=0.05, sd=0.05, nsd=0.3, 
                     max_updates=2000, pstyle='line', psize=1,
                     try_updates=50, sleeptime=0.0, rseed=37, ptitle='Perceptron training progress', 
                    thresh = 0.0):
    """
    Trains a perceptron model and plots the training progress along with a heatmap of inputs and outputs.

    Parameters:
    inputs (np.array): Input data (features with bias included).
    expected_outputs (np.array): Expected output labels.
    learning_rate (float): Learning rate for weight updates.
    sd (float): Standard deviation for random weight initialization.
    max_updates (int): Maximum number of updates/iterations for training.
    sleeptime (float): Time in seconds to pause after each update for dynamic plotting.
    """
    # Set random seed for reproducibility, only if rseed is not None
    if rseed is not None:
        np.random.seed(rseed)

    # Initialize weights
    weights = np.random.normal(0, sd, size=inputs.shape[1])

    # To store total error after each update
    total_errors = []
    total_updates = 0
    # Training loop
    while total_updates < max_updates:
        no_error = True
        for update in range(try_updates):
            # Add noise to inputs
            noisy_inputs = inputs + np.random.normal(0, nsd, inputs.shape)

            total_error = 0
            for input, expected in zip(noisy_inputs, expected_outputs):
                predicted = 1 if np.dot(input, weights) > thresh else 0
                weights += learning_rate * (expected - predicted) * input
                total_error += abs(expected - predicted)
            total_updates += 1

            total_error = 0
            for input, expected in zip(inputs, expected_outputs):
                predicted = 1 if np.dot(input, weights) > thresh else 0
                total_error += abs(expected - predicted)
            total_errors.append(total_error)

            # Dynamic plotting
            clear_output(wait=True)
            fig, ax = plt.subplots(1, 2, figsize=(15, 6))

            # Plot for training progress
            ax[0].set_title(ptitle)
            if pstyle == 'line':
                ax[0].plot(range(total_updates - len(total_errors), total_updates), total_errors, label='Total Error over Updates')
            else:
                ax[0].scatter(range(total_updates - len(total_errors), total_updates), 
                              total_errors, label='Total Error over Updates', s=psize)
            ax[0].set_xlabel('Number of Updates')
            ax[0].set_ylabel('Total Error')
            ax[0].set_xlim(-0.5, ((total_updates // try_updates) + 1) * try_updates)
            ax[0].set_ylim(-0.5, max(total_errors) + 1)
            ax[0].xaxis.set_major_locator(MaxNLocator(integer=True))
            ax[0].grid(True, color='lightgrey')

            # Heatmap for inputs and outputs
            xmax = 1.1
            ymax = 1.1
            observed_outputs = np.array([1 if np.dot(np.append(input[:2], 1), weights) > thresh else 0 for input in inputs])
            cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])
            cmap_bold = ListedColormap(['#FF0000', '#0000FF'])
            xx, yy = np.meshgrid(np.linspace(0, xmax, 100), np.linspace(0, ymax, 100))
            Z = np.array([1 if np.dot(np.array([x, y, 1]), weights) > thresh else 0 for x, y in zip(xx.ravel(), yy.ravel())])
            Z = Z.reshape(xx.shape)
            ax[1].contourf(xx, yy, Z, cmap=cmap_light)
            ax[1].scatter(inputs[:, 0], inputs[:, 1], c=expected_outputs, cmap=cmap_bold, edgecolor='k')
            ax[1].set_xlim(0, xmax)
            ax[1].set_ylim(0, ymax)
            ax[1].set_xlabel('Input 1')
            ax[1].set_ylabel('Input 2')
            ax[1].set_title('Decision boundary')

            plt.show()
            time.sleep(sleeptime)

            if total_error == 0:
                print(f"Converged after {total_updates} updates")
                no_error = True
                break
            else:
                no_error = False

        if no_error:
            break

        if total_updates >= max_updates:
            print(f"Did not converge after {total_updates} updates (max set to {max_updates})")
            break
                        
    # Test the trained perceptron
    for input in inputs:
        predicted = 1 if np.dot(input, weights) > thresh else 0
        print(f"Input: {input[:2]}, Output: {predicted}")

    # Final weights
    print("Final weights:", weights)
    return weights           

inputs = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
or_expected_outputs = np.array([0, 0, 0, 1])  # AND outputs


or_wts = train_perceptron(inputs, or_expected_outputs, learning_rate=0.05, sd=2.0, 
                           try_updates=50, max_updates=4000, rseed=None)








import network_plotter as netplot
import numpy as np
import matplotlib.pyplot as plt

# Setup for the AND network plot
plot_width = 10.0
plot_height = plot_width
fig_width = plot_width * 1.2
fig_height = fig_width
figsize = (fig_width, fig_height)

# Define the network for the AND function
layers_and = [
    ['Input 1', 'Input 2', 'Bias'],
    ['Output']
]

# Set weights for the AND function
# Both inputs need to be 1 to exceed the threshold of 1.5
#input_to_output_weights_and = np.array([1.0, 1.0, -1.5])

# Instead of setting the weights, we will use the ones we just got from training. 
input_to_output_weights_and = or_wts


# Creating the figure for the AND network
fig, ax = plt.subplots(figsize=figsize)
ax.set_xlim(0, fig_width)
ax.set_ylim(0, fig_height)
ax.set_aspect('equal', adjustable='box')

# Calculate node positions for the AND network
node_positions_and = netplot.calculate_node_positions(layers_and, plot_width, fig_width, fig_height)

# Draw Neurons with Labels for the AND network
num_layers_and = len(layers_and)
subtext_and = ''
nodetext_and = ''
for i, column in enumerate(layers_and):
    for node in column:
        pos = node_positions_and[node]
        # if i == num_layers_and - 1:
        #     subtext_and = 'Threshold: 1.5'
        netplot.draw_neuron_with_label(ax, pos, node, i, num_layers_and, plot_width=plot_width, subtext=subtext_and, nodetext=nodetext_and)

# Draw Connections with Weights for the AND network
for i, input_node in enumerate(layers_and[0]):
    weight = input_to_output_weights_and[i]
    netplot.draw_connection_with_weight(ax, node_positions_and, input_node, 'Output', weight, show_weight=True, plot_width=plot_width)

plt.title('AND network with bias node and trained weights', x=0.5, y=0.85)
plt.axis('off')
plt.show()









or_expected_outputs = np.array([0, 1, 1, 1])  # OR outputs


or_wts = train_perceptron(inputs, or_expected_outputs, learning_rate=0.1, sd=2.0, try_updates=50, max_updates=4000, rseed=None)







import pandas as pd
import time

# Initialize the list to store successful weights
successful_weights = []

# Run the training 10 times
for run in range(10):
    or_wts = train_perceptron(inputs, or_expected_outputs, ptitle=f'Run {run}',
                              learning_rate=0.5, sd=2.0, try_updates=50, max_updates=4000, rseed=None)
    # Separate the larger and smaller weights and append them with the bias
    larger_weight = max(or_wts[0], or_wts[1])
    smaller_weight = min(or_wts[0], or_wts[1])
    successful_weights.append([larger_weight, smaller_weight, or_wts[2]])
    time.sleep(1)

# Create a DataFrame with columns for larger weight, smaller weight, and bias
weights_df = pd.DataFrame(successful_weights, columns=['Larger_Weight', 'Smaller_Weight', 'Bias'])

# Calculate the mean of the larger and smaller weights separately, along with the bias
mean_weights = pd.DataFrame([weights_df.mean()], columns=weights_df.columns)
mean_weights.index = ['Mean']

# Concatenate the mean row to the DataFrame
weights_df_with_mean = pd.concat([weights_df, mean_weights])
weights_df_with_mean









import pandas as pd
import time

# Initialize the list to store successful weights
successful_weights = []

# Run the training 10 times
for run in range(10):
    or_wts = train_perceptron(inputs, or_expected_outputs, ptitle=f'Run {run}',
                              learning_rate=0.5, sd=2.0, try_updates=50, max_updates=4000, rseed=None)
    # Separate the larger and smaller weights and append them with the bias
    larger_weight = max(or_wts[0], or_wts[1])
    smaller_weight = min(or_wts[0], or_wts[1])
    successful_weights.append([larger_weight, smaller_weight, or_wts[2]])
    time.sleep(1)

# Create a DataFrame with columns for larger weight, smaller weight, and bias
weights_df = pd.DataFrame(successful_weights, columns=['Larger_Weight', 'Smaller_Weight', 'Bias'])

# Calculate the mean of the larger and smaller weights separately, along with the bias
mean_weights = pd.DataFrame([weights_df.mean()], columns=weights_df.columns)
mean_weights.index = ['Mean']

# Concatenate the mean row to the DataFrame
weights_df_with_mean = pd.concat([weights_df, mean_weights])
weights_df_with_mean






import pandas as pd
import numpy as np
import time

nor_expected_outputs = np.array([1, 1, 1, 0])  # NAND outputs

# Initialize the list to store successful weights
successful_weights = []

# Run the training 10 times
for run in range(10):
    or_wts = train_perceptron(inputs, nor_expected_outputs, ptitle=f'Run {run}',
                              learning_rate=0.5, sd=2.0, try_updates=50, max_updates=4000, rseed=None)
    # Separate the larger and smaller weights and append them with the bias
    larger_weight = max(or_wts[0], or_wts[1])
    smaller_weight = min(or_wts[0], or_wts[1])
    successful_weights.append([larger_weight, smaller_weight, or_wts[2]])
    time.sleep(1)

# Create a DataFrame with columns for larger weight, smaller weight, and bias
weights_df = pd.DataFrame(successful_weights, columns=['Larger_Weight', 'Smaller_Weight', 'Bias'])

# Calculate the mean of the larger and smaller weights separately, along with the bias
mean_weights = pd.DataFrame([weights_df.mean()], columns=weights_df.columns)
mean_weights.index = ['Mean']

# Concatenate the mean row to the DataFrame
weights_df_with_mean = pd.concat([weights_df, mean_weights])
weights_df_with_mean









import pandas as pd
import numpy as np
import time

xor_expected_outputs = np.array([0, 1, 1, 0])  # XOR outputs

# Initialize the list to store successful weights
successful_weights = []

# Run the training just 3 times, since we know it will not converge
for run in range(3):
    or_wts = train_perceptron(inputs, xor_expected_outputs, ptitle=f'Run {run}',
                              learning_rate=0.05, sd=2.0, try_updates=50, max_updates=200, rseed=None)
    # Separate the larger and smaller weights and append them with the bias
    larger_weight = max(or_wts[0], or_wts[1])
    smaller_weight = min(or_wts[0], or_wts[1])
    successful_weights.append([larger_weight, smaller_weight, or_wts[2]])
    time.sleep(1)

# Create a DataFrame with columns for larger weight, smaller weight, and bias
weights_df = pd.DataFrame(successful_weights, columns=['Larger_Weight', 'Smaller_Weight', 'Bias'])

# Calculate the mean of the larger and smaller weights separately, along with the bias
mean_weights = pd.DataFrame([weights_df.mean()], columns=weights_df.columns)
mean_weights.index = ['Mean']

# Concatenate the mean row to the DataFrame
weights_df_with_mean = pd.concat([weights_df, mean_weights])
weights_df_with_mean






import matplotlib.pyplot as plt
import numpy as np

# Plotting the inputs
for point, output in zip(inputs, xor_expected_outputs):
    if output == 1:
        plt.scatter(point[0], point[1], color='blue', marker='o', s=100)  # Output 1
    else:
        plt.scatter(point[0], point[1], color='red', marker='x', s=100)  # Output 0

# Annotations and labels
plt.title('XOR inputs and outputs ')
plt.xlabel('Input 1')
plt.ylabel('Input 2')
plt.xlim(-1, 2)
plt.ylim(-1, 2)
plt.grid(True, color='lightgrey')
plt.show()






import matplotlib.pyplot as plt
import network_plotter as netplot
import numpy as np

# Define the network for XOR function
layers_xor = [
    ['Input 1', 'Input 2', 'Bias Hid'],
    ['Hid 1', 'Hid 2', 'Bias Out'],
    ['Output']
]

# Set weights for the XOR function
input_to_hidden1_weights_xor = np.array([1.0, 1.0, -0.5])
input_to_hidden2_weights_xor = np.array([-1.0, -1.0, 1.5])
hidden_to_output_weights_xor = np.array([1.0, 1.0, -1.5])
bias_to_output_weight = -1.5

# Setup for the network plot
plot_width = 7.0
plot_height = plot_width
fig_width = plot_width * 1.2
fig_height = fig_width
figsize = (fig_width, fig_height)

# Creating the figure for the XOR network
fig, ax = plt.subplots(figsize=figsize)
ax.set_xlim(0, fig_width)
ax.set_ylim(0, fig_height)
ax.set_aspect('equal', adjustable='box')

# Calculate node positions for the XOR network
node_positions_xor = netplot.calculate_node_positions(layers_xor, plot_width, fig_width, fig_height)

# Draw Neurons with Labels for the XOR network
num_layers_xor = len(layers_xor)
subtext_xor = ''
nodetext_xor = ''
for i, column in enumerate(layers_xor):
    for node in column:
        pos = node_positions_xor[node]
        netplot.draw_neuron_with_label(ax, pos, node, i, num_layers_xor, plot_width=plot_width, subtext=subtext_xor, nodetext=nodetext_xor)

# Draw Connections with Weights for the XOR network
# Input to Hidden Layer 1
for i, input_node in enumerate(layers_xor[0]):
    weight = input_to_hidden1_weights_xor[i]
    netplot.draw_connection_with_weight(ax, node_positions_xor, input_node, 'Hid 1', weight, show_weight=True, plot_width=plot_width)

# Input to Hidden Layer 2
for i, input_node in enumerate(layers_xor[0]):
    weight = input_to_hidden2_weights_xor[i]
    netplot.draw_connection_with_weight(ax, node_positions_xor, input_node, 'Hid 2', weight, show_weight=True, plot_width=plot_width)

# Hidden to Output Layer
for i, hidden_node in enumerate(layers_xor[1]):
    weight = hidden_to_output_weights_xor[i]
    netplot.draw_connection_with_weight(ax, node_positions_xor, hidden_node, 'Output', weight, show_weight=True, plot_width=plot_width)

# Adding the connection from Bias to Output
#bias_to_output_weight = hidden_to_output_weights_xor[-1]
#netplot.draw_connection_with_weight(ax, node_positions_xor, 'Bias', 'Output', bias_to_output_weight, show_weight=True, plot_width=plot_width)
   
plt.title('XOR network with bias node and hidden layers', x=0.5, y=0.9)
plt.axis('off')
plt.show()





def step_activation(x):
    return np.where(x > 0, 1, 0)

def compute_xor_network(inputs, weights):
    # Unpack weights
    w_input_hidden1, w_input_hidden2, w_hidden_output = weights

    for input in inputs:
        # Calculate the initial states (sums) for Hid 1 and Hid 2
        hid1_sum = np.dot(input, w_input_hidden1)
        hid2_sum = np.dot(input, w_input_hidden2)

        # Apply step activation function to get the final output values for Hid 1 and Hid 2
        hid1_output = step_activation(hid1_sum)
        hid2_output = step_activation(hid2_sum)

        # Calculate the raw input to the Output node
        output_sum = hid1_output * w_hidden_output[0] + hid2_output * w_hidden_output[1] + 1 * w_hidden_output[2]

        # Apply step function to get the final Output value
        final_output = step_activation(output_sum)

        # Print the input values, raw and stepped values for Hid 1, Hid 2, and Output
        print(f"Input: {input}, Hid 1: ({hid1_sum}, {hid1_output}), Hid 2: ({hid2_sum}, {hid2_output}), Output: ({output_sum}, {final_output})")

# Inputs and weights remain the same as the previous example
weights_xor = [
    input_to_hidden1_weights_xor,
    input_to_hidden2_weights_xor, 
    hidden_to_output_weights_xor
]

# Running the XOR network with the corrected logic
compute_xor_network(inputs, weights_xor)






